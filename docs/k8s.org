* General
- Kubernetes has a cloud provider interface that allows any cloud provider to implement it and integrate Kubernetes seamlessly
* K8S Node
- Each Kubernetes node runs several Kubernetes components, such as a kubelet and a kube proxy. Nodes are managed by a Kubernetes master.
** Node maintenance : done by *Node controller*
- assigns *IP space*
- updates node list with available machines
- monitor health of node
  - unhealthy node gets deleted
  - Pods which were running gets rescheduled to another node
- on adding a new node
  - kubelet try to register itself *self registration*
- new Node object is created
  - metadata
  - labels
- Node condition (Ready, OutOfDisk)
- decommision a node gracefully
  - drain a node ~kubectl dain nodename --grace-period=6000~
** Components
- Proxy: networking
- kubelet: oversees communication with master
* K8S master
[[file:./assets/master-architecture.png]]
** REST / Server
- Provides REST APIs
- Stores the definitions on =etcd=
** etcd
** Scheduler
- Takes care of intiating deployments on nodes
** Controller Manager : Various kinds of controllers
- Node controller
- Replication controller
- ......
** DNS
- as on 1.3, DNS service is part of cluster
* pod basics
- describes app running on container
- 1 or more tightly coupled containers that makes up the app
- Always run on the same machine.
- All containers have same IP and port space
- Apps can communicate with each other using local port numbers
** sample commands run
#+BEGIN_SRC shell
kubectl get pod

kubectl describe pod nodehelloworld.example.com

#host port:container port
kubectl port-forward nodehelloworld.example.com 8081:3000
Forwarding from 127.0.0.1:8081 -> 3000
Forwarding from [::1]:8081 -> 3000
Handling connection for 8081

kubectl expose pod nodehelloworld.example.com --type=NodePort --name nodehelloworld-service
service "nodehelloworld-service" exposed

minikube service nodehelloworld-service --url
http://192.168.99.102:31475

kubectl get service

kubectl attach nodehelloworld.example.com
If you don't see a command prompt, try pressing enter.

kubectl exec nodehelloworld.example.com -- ls /app

kubectl describe service nodehelloworld-service
#+END_SRC
** manage a pod
#+BEGIN_SRC 
kubectl create -f ./pod-helloworld.yml

kubectl get pods
#+END_SRC
** useful pod commands
[[file:./assets/podCommands.png]]
* Labels
- key-value pairs similiar to tags in AWS
- can be used to filter using *Label Selectors*
- The label key must adhere to a strict syntax.
  - It has two parts: prefix (optional) and name.
  - If it exists then it is separated from the name by a forward slash (/) and it must be a valid DNS sub-domain.
  - The prefix must be 253 characters long at most
- used on objects ~N X N~ relationship
  - pods
  - nodes
    - tag a node
    - add *nodeSelector* to the pod configuration to run pod only on selected nodes
    - pods will be in *pending state* until matching nodes are available
#+BEGIN_SRC 
kubectl label nodes node1 hardware=high-spec
kubectl label nodes node1 hardware=low-spec

#in pod yml under spec
nodeSelector:
  hardware: high-spec
kubectl get nodes --show-labels
NAME       STATUS    AGE       LABELS
minikube   Ready     50m       beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=minikube

kubectl label nodes minikube hardware=high-spec, application != foo
node "minikube" labeled
#+END_SRC

* Annotations
- Annotations let you associate arbitrary metadata with Kubernetes objects
* scaling
- Replication controllers and replica sets both manage a group of pods identified by a label selector and ensure that a certain number is always up and running
** scaling options
- *stateless* apps can be scaled *horizontally*
- *stateful* can be scaled *vertically*
** can be done using *replication controller*
- enusre specified number of pod replicas are running all the time
- automatic recreation on termination
** commands run
- scale with file
#+BEGIN_SRC 
gabbi@Gauravs-Air  ~/learning/udemy/kubernetes/code  kubectl scale --replicas=4 -f ./pod-replication-controller.yml
replicationcontroller "helloworld-controller" scaled
#+END_SRC
- scale using controller name
#+BEGIN_SRC 
 gabbi@Gauravs-Air  ~/learning/udemy/kubernetes/code  kubectl get rc
NAME                    DESIRED   CURRENT   READY     AGE
helloworld-controller   4         4         4         9m
 gabbi@Gauravs-Air  ~/learning/udemy/kubernetes/code  kubectl scale --replicas=1 rc/helloworld-controller
replicationcontroller "helloworld-controller" scaled
 gabbi@Gauravs-Air  ~/learning/udemy/kubernetes/code  kubectl get pods
NAME                               READY     STATUS        RESTARTS   AGE
hello-minikube1-1849020499-71npc   1/1       Running       0          14m
helloworld-controller-2nm50        1/1       Running       0          9m
helloworld-controller-95g66        1/1       Terminating   0          5m
helloworld-controller-b4b7z        1/1       Terminating   0          7m
helloworld-controller-n06pr        1/1       Terminating   0          5m
#+END_SRC
** *Replication set* supports new selector
- selection based on ~filtering~ according to a set of ~values~
- used by ~Deployment Object~
* Services
- _Service is the logical bridge between pods and other services or end-users_
- operates at level 3 (TCP/UDP)
- Pods are very dynamic.They come and go on the kubernetes cluster
  - Replication controller : pods are terminated and created during scaling
  - Deployments : while updating an image version, podsa are terminated and new pods take their place
- =kubectl expose= creates a new service for the pod
- creating a service : creates endpoint for the pod
  - *ClusterIP* : virtual ip address reachable from within the cluster
    - it is dynamic
    - if need static, it should be defined in yml file for service
  - *NodePort* : same on each port that is reachable externally
  - *LoadBalancer* : from *cloud provider* will route external traffic to every node on the NodePort
  - DNS Names
    - *ExternalName* can provide DNS name for the service
    - *DNS add-ons* needs to be enabled

#+BEGIN_SRC 
kubectl create -f service-NodePort.yml
service "helloworld-node-port" created

minikube service helloworld-node-port --url
http://192.168.99.100:31001

http http://192.168.99.100:31001
HTTP/1.1 200 OK

kubectl describe service helloworld-node-port
Name:                   helloworld-node-port
Namespace:              default
Labels:                 <none>
Selector:               app=helloworld
Type:                   NodePort
IP:                     10.0.0.3
Port:                   <unset> 31001/TCP
NodePort:               <unset> 31001/TCP
Endpoints:              172.17.0.4:3000
Session Affinity:       None
No events.
#+END_SRC
* Volumes
** how it works
[[file:./assets/use-volumes.png]]
** for stateful apps
** Persistent Volumes in kubernetes allow to attach volumes to containers that exist even after container is destroyed
** If node stops working, pod is scheduled to another node and same volume is used
- *works only* for nodes in the *same availability zone*
** Aws Ebs example
[[file:./assets/volume-aws-ebs.png]]
** Provisioning
- *AWS Plugin* can provision storage. It create volume before attaching them to a container/node.
- using *StorageClass*
- define storage
  - [[file:./assets/volume-storage-class.png]]
- volume claim
  - [[file:./assets/volume-claim.png]]
- use volume claim in pod
  - [[file:./assets/volume-claim-in-pod.png]]
** types
- /emptyDir/
  - starts empty
  - ephemeral
  - stores data on node (memory or disk)
#+BEGIN_SRC yaml
  # declare
  volumes:
  - name: cache-volume
    emptyDir: {}

  # use
  name: demo
  image: cloudnatived/demo:hello
  volumeMounts:
  - mountPath: /cache
    name: cache-volume
#+END_SRC

- /PersistentVolume/
  - pool
  - *dynamic provisioning*
#+BEGIN_SRC yaml
# declare
volumes:
- name: data-volume
  persistentVolumeClaim:
    claimName: data-pvc
#+END_SRC
* StatefulSet
- to manage a distributed data store
- Properties
  - A stable hostname, available in DNS
  - An ordinal index
  - Stable storage linked to the ordinal and hostname
* Secrets
- stored as plaintext in etcd
- distribute *credentials, keys, passwords or secret data* to pods
- used by k8s for internal APIs
- can be used to provide secrets to application
- one can use _other ways_ if not using *Secrets* e.g. /external vault services/
- Usage patterns
  - as *environment variables*
    - [[file:./assets/secrets_env_pod.png]]
  - as a *file* in pod
    - [[file:./assets/secrets_volume_pod.png]]
    - uses *volumes* mounted on container which has *files*
    - *files* can contain *dotenv* or other files based on how secrets are read
  - from *external second image*
** Generation
- using command line
- using yml file
* Namespaces
- allow to create virtual clusters
- logically separate the cluster
- OOB namespaces
  - /default/
  - /kube-system/
- commands
  - =kubectl get namespaces=
  - =kubectl create namespace myspace=
  - set default namespace to launch resources in
    - =export context = $(kubectl config view|awk '/current-context/{print $2}')=
    - =kubectl config set-context $CONTEXT --namespace=myspace=
- Using namespaces to set quotas
  - [[file:./assets/namespace-resource-quotas.png]]  [[file:./assets/namespace-object-quotas.png]]
* Distributed System Design patterns
** Side car pattern
- about co-locating another container in a pod in addition to the main application container.
- e.g. a container to send logs to central logging service
** Ambassador pattern
- representing a remote service as if it were local and possibly enforcing some policy.
- Redis cluster with one master for writes and many replicas for reads.
  - A local ambassador container can serve as a proxy and expose Redis to the main application container on the localhost.
** ADAPTER PATTERN
- standardizing output from the main application container
** Multi-node patterns
- patterns such as leader election, work queues, and scatter-gather are not supported directly,
- but composing pods with standard interfaces to accomplish them is a viable approach with Kubernetes
* APIs
** Kubernetes APIs
** Autoscaling API ~/apis/autoscaling/v1~
** Batch API ~/apis/batch/v1~
* K8S runtimes ~runtime interface~
- ACI (App Container Images) vs OCI (Open Container Initiative)
- Dokcer
                                                         ~-----> runC~
  - ~docker commnand ---> Docker engine -----> containerd -----> runC~
                                                         ~-----> runC~
- Rkt
  - features
    - security and isolation
    - No daemon
    - relies on OS init system
  - App container
  - RKTNETES
* K8S for CI/CD deployment
- a set of steps that a set of changes by developers or operators that modify the code, data or configuration of a system, test them and deploys them to production.
* Monitoring K8S
- cAdvisor collects information about CPU cores , memory, filesystem usage
- cAdvisor UI runs on port 4194
- provides all information to heapster via kubelet
** Resource Usage monitoring
*** how it works
[[file:./assets/resoure-monitoring.png]]
*** Heapster
- enables *Container Cluster Monitoring* *Performance Analysis*
- pre-requisite for *pod auto-scaling*
- export metrics via *REST*
- can be used with differnt backends (InfluxDB, Kafka and others)
- Grafana for view
#+BEGIN_SRC 
minikube service monitoring-grafana --namespace=kube-system --url
http://192.168.99.100:30869
#+END_SRC
- Kubernetes dashboard will also show graphs
* minikube
#+BEGIN_SRC shell
  minikube start --memory=8192 --cpus=4 \
    --kubernetes-version=v1.13.0 \
    --vm-driver=hyperkit \
    --bootstrapper=kubeadm \
    --feature-gates=ExpandPersistentVolumes=true \
    --extra-config=apiserver.enable-admission-plugins="LimitRanger,NamespaceExists,NamespaceLifecycle,ResourceQuota,ServiceAccount,DefaultStorageClass,MutatingAdmissionWebhook,PersistentVolumeClaimResize"
#+END_SRC
* kubectl
** create deployment
#+BEGIN_SRC 
 gabbi@Gauravs-Air  ~/learning/udemy/kubernetes  kubectl run hello-minikube1 --image=gcr.io/google_containers/echoserver:1.4 --port=8080
 deployment "hello-minikube1" created
#+END_SRC
** expose deployment
#+BEGIN_SRC 
 gabbi@Gauravs-Air  ~/learning/udemy/kubernetes  kubectl expose deployment hello-minikube1 --type=NodePort
service "hello-minikube1" exposed
#+END_SRC
** get service url
#+BEGIN_SRC 
gabbi@Gauravs-Air  ~/learning/udemy/kubernetes  minikube service hello-minikube1 --url
http://192.168.99.100:32619
#+END_SRC
* AWS setup
** kops (*Kubernetes Operations*) used for installations, upgrades and management
** Steps on AWS [0/5]
- [-] create a kops user using *IAM*
- [ ] assign administrator policy
- [ ] create a bucket based on region where the cluster is to be installed
- [ ] get a free domain from somewhere
- [ ] use *Amazon Route 53* for DNS
  - [ ] create hosted zone
* architecture
** diagram
[[file:./assets/kubernetes-architecture.png]]
** node architecture
* Deployment object
** declaration that allows to do app deployments and updates
** when using ~deployment object~, one can define state of the app
- kubernetes than make sure that the clusters matches your desired state
** with DO, one can
- create
- update
- rolling update
- roll back
- Pause/resume
** commands reference
[[file:./assets/deploymentCommands.png]]
** commands run
#+BEGIN_SRC 
gabbi@Gauravs-Air  ~/learning/udemy/kubernetes/code  kubectl get deployments
NAME              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
hello-minikube1   1         1         1            1           6d
 gabbi@Gauravs-Air  ~/learning/udemy/kubernetes/code  kubectl get rs
NAME                         DESIRED   CURRENT   READY     AGE
hello-minikube1-1849020499   1         1         1         6d
gabbi@Gauravs-Air  ~/learning/udemy/kubernetes/code 

gabbi@Gauravs-Air  ~/learning/udemy/kubernetes/code  kubectl rollout status deployment/helloworld-deployment
deployment "helloworld-deployment" successfully rolled out
 gabbi@Gauravs-Air  ~/learning/udemy/kubernetes/code  kubectl expose deployment helloworld-deployment --type=NodePort
service "helloworld-deployment" exposed
 gabbi@Gauravs-Air  ~/learning/udemy/kubernetes/code  kubectl get service
NAME                    CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE
helloworld-deployment   10.0.0.243   <nodes>       3000:31425/TCP   25s
kubernetes              10.0.0.1     <none>        443/TCP          7d
 gabbi@Gauravs-Air  ~/learning/udemy/kubernetes/code  kubectl describe service helloworld-deployment
Name:                   helloworld-deployment
Namespace:              default
Labels:                 app=helloworld
Selector:               app=helloworld
Type:                   NodePort
IP:                     10.0.0.243
Port:                   <unset> 3000/TCP
NodePort:               <unset> 31425/TCP
Endpoints:              172.17.0.5:3000,172.17.0.6:3000,172.17.0.7:3000 + 1 more...
Session Affinity:       None
No events.
gabbi@Gauravs-Air  ~/learning/udemy/kubernetes/code  minikube service helloword-deployment --url
service 'helloword-deployment' could not be found running in namespace 'default' within kubernetes
 ✘ gabbi@Gauravs-Air  ~/learning/udemy/kubernetes/code  minikube service helloworld-deployment --url
http://192.168.99.102:31425
#+END_SRC
** edit deployment
#+BEGIN_SRC 
kubectl edit deployment/healthcheck-deployment
#+END_SRC
- it opens a editor where we can change the settings and save them
- *might not change the actual source file*
* health checks
** to detect application malfunctions
** 2 types
- a periodic command in the container
- periodic checks on the URL (HTTP)
** if failed, pod is terminated and new one is launched
* Web UI
** To access
#+BEGIN_SRC 
minikube dashboard --url
http://192.168.99.100:30000
#+END_SRC
** provides similiar functionality as _kubectl_
** commands
#+BEGIN_SRC sh
  kubectl create -f https://<path to kubernetes-dashboard>

  #to see the password
  kubectl config view
#+END_SRC
* Service Discovery
** DNS is a built-in service since K8s 1.3
[[file:./assets/dns.png]]                      [[file:./assets/dns-internal.png]]   
- can be used to find other services running on same cluster
- to make it work, pod will need a service defintion
- containers in same pod can contact each other using localhost:port. Don't need DNS
- use =nslookup= to get the IP
* configMap
** can be used for non-secret config parameters
** *key-value pairs*
** can be read by the app using
- Environment variables
  - [[file:./assets/configMap-using-env.png]]
- container command line arguments in pod configurations
- volumes
  - [[file:./assets/configMap-specify-in-pod.png]]
- can also contain a full configuration *file* 
  - [[file:./assets/configMap-using-file.png]]
  - file then can be mounted using volumes
** commands
#+BEGIN_SRC sh
  kubectl create configmap nginx-config --from-file=./configMap/reverseproxy.conf

  kubectl get configmap

  kubectl get configmap nginx-config -o yaml
#+END_SRC
* ingress
[[file:./assets/ingress.png]]
** allows *inbound connections* to the cluster
** alternative to load balancer and Node ports
- expose services to be accessed from outside cluster
** run your own *ingress controller*
** default ingress controller or one can write own
* PetSets Stateful distributed apps on K8s cluster
** *stable/static Pod hostname*
- podname-0, podname-1
** stateful apps that require multiple volumes based on ordinal number
- deleting/scaling down PetSet will not delete volume
** allow app to use DNS to find other servers
- One running node of a PetSet is called *Pet*
** _order startup and teardown of pets_
* Daemon Sets
** ensure that each node on the cluster runs the same pod resource
** useful when one wants to run certains pods on each node
** node starts, also start the pod
** node stops, do not reschdule the pod on another node
** use cases
- logging aggregators
- monitoring
- load balancers / proxy / api gateway
- any other similiar app per host
** example
[[file:./assets/daemon-set.png]]
* Auto Scaling
** based on metrics *by heapster*
** deployment, replication controller, replica set
** scaling based on CPU is OOB
- also possible for custom metrics
- ENABLE_CUSTOM_METRICS

* Resource Quotas
** control the resources allocated to teams
** deivide cluster into namespaces and assing quotas to it.
** Use objects
- ResoureQuotas
- ObjectQuotas
** container can specify
- request capacity : minimum capacity pod needs
- capacity limits : limits beyond which container cannot utilize resources
** Resource limits that can be set by admin
[[file:./assets/resource-quotas-admin.png]]
** Object quotas that can be set by admin
[[file:./assets/object-quotas-admin.png]]
* User Management
- ~system:authenticated~ group for authenticated user
** Normal user
- to connect from external to K8S. e.g. from =kubectl=
- Not managed like K8S object
- Authentication
  - Client certificates
  - Authorization Proxy
  - OpenID
  - HTTP Basic Authentication
  - Bearer Token
  - Webhooks (external service)
- authorization
  - Alwaysallow/AlwaysDeny
  - ABAC (Attribute based access control)
  - RBAC (Role based access control)
  - Webhook
** Demo
#+BEGIN_SRC sh
  openssl genrsa -out myuser.pem 2048
  openssl req -new -key myuser.pem -out myuser-csr.pem -subj "/CN=myuser/O=myteam/"
  openssl x509 -req -in myuser-csr.pem -CA ~/.minikube/ca.crt -CAkey ~/.minikube/ca.key -CAcreateserial -out myuser.crt -days 10000
#+END_SRC
** client certificates : ~CN of the subject is used as the user name~
** bearer token :
- ~static token file~
- format : token,user,uid,"group1,group2,group3"
- in HTTP requests ~Authorization: Bearer <token>~
- ~Bootstrap Tokens~
  - ~alpha~
  - dynamically-managed Bearer token 
** authenticating proxy
- Header ~X-Remote-User~
** HTTP Basic Auth
- static file for basic auth
- format : password,user,uid,"group1,group2,group3"
** service token tokens
- automatically enabled authenticator
- ~sa~ created automatically by the API server and associated with pods running in the cluster through the ServiceAccount
- Accounts may be explicitly associated with pods using the ~serviceAccountName~ field of a ~PodSpec~
- for in-cluster access to the API server
- can also be used from outside
** OpenID Connect Tokens
- ~ID Token~ is a JWT
- authenticator uses ~id_token~
** Webhook Token Authentication
** anonymous requsts
- user ~system:anonymous~
- group ~system:unauthenticated~
** user impersonation
- API server ensures the authenticated users has impersonation privileges.
- ~Impersonate-User~
- ~Impersonate-Group~
- ~Impersonate-Extra-( extra name )~
* Networking
** container to container
- using localhost and the port number
** pod to service
- using NodePort and DNS
** external to service
- using LoadBalancer, NodePort
** pod to pod
- K8S assumes it is always possible, no matter on which node pods are running
- each pod has its own IP address
** set up
|-----+-----------------------------------------------------------------|
| AWS | kubenet                                                         |
|     | - each pod gets ip, that is routable using AWS VPC              |
|     | - k8s master allocates /24 subnet to each node (254 IP addresses) |
|     | - subnet is added to VPC table                                  |
|     | - *50 enteries* limit                                           |
|-----+-----------------------------------------------------------------|
|     | CNI (Container Network Interfaces)                              |
|     | - Calico, Weave                                                 |
|-----+-----------------------------------------------------------------|
|     | An Overlay network                                              |
|     | - Flannel                                                       |
[[file:./assets/flannel.png]]
* operators
- an application-specific controller that extends the Kubernetes API to create, configure, and manage instances of complex stateful applications on behalf of a Kubernetes user.
- encodes the domain knowledge and extends the Kubernetes API through the ~CRDs~ mechanism
* HA
[[file:./assets/ha.png]]
- Clustering etcd
- replicated API servers with LoadBalancers
- multiple instances of scheduler and controller
  - only one of them is leader, others are stand-by
** commands
#+BEGIN_SRC sh 
  kops create cluster --name=kubernetes.newtech.academy --state=s3://kops-state-b429b --zones=eu-west-1a --node-count=2 --node-size=t2.micro --master-size=t2.micro --dns-zone=kubernetes.newtech.academy
  kops update cluster kubernetes.newtech.academy --yes --state=s3://kops-state-b429b
  kops delete cluster --name kubernetes.newtech.academy --state=s3://kops-state-b429b
  kops delete cluster --name kubernetes.newtech.academy --state=s3://kops-state-b429b --yes
#+END_SRC
* Istio
[[file:./assets/istio-architecture.png]]
- provides an easy way to create a network of deployed services with
  - load balancing,
  - service-to-service authentication,
  - monitoring, and more,
  - without requiring any changes in service code.
- done by deploying a side-car proxy
  - throughout your environment that intercepts all network communication between microservices, configured and managed using Istio’s control plane functionality.
- *Components*
  - data plane and control plane
  - Envoy : Proxies deployed as sidecars
  - Mixer : Enforcing access control and usage policies and collecting telemetry data
  - Pilot : collecting and validating configuration and propagating it to the various Istio components.
  - Istio-auth : strong service-to-service and end-user authentication using mutual TLS, with built-in identity and credential management
- commands
  - ~kubectl apply -f <(istioctl kube-inject -f uaa/uaa.yml)~
    - injects additional containers into YAML resource on the client before submitting to the Kubernetes API server.
    - This will eventually be replaced by server-side injection via admission controller.
* K8S objects
- intent for the desired state of the cluster
- two nested object fields
  - spec   : desired state for the object
  - status : actual state of the object
- At any given time, the Kubernetes Control Plane actively manages an object’s actual state to match the desired state you supplied.
* custom resources
- Custom resources
  - A resource is an endpoint in the Kubernetes API that stores a collection of API objects of a certain kind.
  - A custom resource is an extension of the Kubernetes API that is not necessarily available on every Kubernetes cluster.
  - objects for custom resources can be built using kubectl
- Custom controllers
  - when custom resources are combined with custom controllers, they become true declarative api
  - else they are just stores and retrieved as some structured data
  - can work with any kind of resources
  - operator pattern : It allows developers to encode domain knowledge for specific applications into an extension of the Kubernetes API.
- Custom resources definition (CRD)
- Aggregation
  - The aggregation layer allows you to provide specialized implementations for your custom resources by writing and deploying your own standalone API server.
  - The main API server delegates requests to you for the custom resources that you handle, making them available to all of its clients.
* Helm charts
- Helm is the package manager (analogous to yum and apt) and
- Charts are packages (analogous to debs and rpms).
* K8S 1.9
- ClusterRole aggregation
  - ClusterRole can be reused across ClusterRoles
- Network policy
  - ingress and egress rules
- CRDs can be validated
  - using OpenAPI scheme
  - type enforcement at the API server
- mutating admission webhook
  - custom behavior when API
  - e.g. inject sidecar to a pod definition before storage
- IPVS : alpha
- IP6 : alpha
- container storage interface : alpha
  - volume plugins outside of k8s core
- CoreDNS : alpha
- Device plugins : alpha
  - hardware acceleration : alpha
- block devices can be used as volumes : alpha
* RBAC
- uses ~rbac.authorization.k8s.io~ API group
- Role represent rules that represent set of permissions
  - within namespace with a ~Role~ or cluster wide using ~ClusterRole~
  - ~permissions~ are purely additive
- To enable, start the API server with ~-authorization-mode=RBAC~
- Aggregated ClusterRoles
** binding to ~subject~
- groups
- users
- servcie accounts
  - username have ~system:serviceaccount:~ prefix
  - belong to groups ~system:serviceaccounts:~ prefix
- for all sa in qa namespace
#+BEGIN_SRC yaml
    subjects:
  - kind: Group
    name: system:serviceaccounts:qa
    apiGroup: rbac.authorization.k8s.io
#+END_SRC
- for all authenticated users
#+BEGIN_SRC yaml
  subjects:
  - kind: Group
    name: system:authenticated
    apiGroup: rbac.authorization.k8s.io
#+END_SRC
* useful k8s commands
- /kubectl explain/
  - ~kubectl explain pods~
  - ~kubectl explain deploy.spec.template.spec.containers.livenessProbe.exec~
  - ~kubectl explain --recursive <resources>~
- /kubectl export/
  - ~kubectl get deployments newdemo -o yaml --export >deployment.yaml~
- /kubectl alpha diff/
  - ~kubectl alpha diff -f deployment.yaml~
- /kubectl logs --tail=x/
  - ~kubectl logs --namespace kube-system --tail=10 --follow etcd-docker-for-desktop~
- /kubect attach/
  - ~kubectl attach demo-54f4458547-fcx2n~
- /kubectl exec/
  - ~kubectl exec -it alpine-7fd44fc4bf-7gl4n /bin/sh~
- /running comtainers for troubleshooting/
  - ~kubectl run nslookup --image=busybox:1.28 --rm -it --restart=Never --command -- nslookup demo~
* k8s tools
- kubesquash
- kubespy
- kubectx
- kubens
- kube-ps1
- kube-shell
- kubed-sh
- Stern
* K8S security
- Pod security context
#+BEGIN_SRC yaml
  securityContext: # container and pod level
    runAsUser: 1000
    runAsNonRoot: true
    readOnlyRootFilesystem: true  # container cannot write to its filesystem
    allowPrivilegeEscalation: false
    capabilities:
      drop: ["all"]
      add: ["NET_BIND_SERVICE"]
#+END_SRC
- /Pod Security policies/
  - cluster level
#+BEGIN_SRC yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: example
spec:
  privileged: false
  # The rest fills in some required fields.
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  runAsUser:
    rule: RunAsAny
  fsGroup:
    rule: RunAsAny
  volumes:
  - *
#+END_SRC

